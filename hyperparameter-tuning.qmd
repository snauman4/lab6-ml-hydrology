---
title: "Lab 8: Machine Learning in Hydrology"
subtitle: "Hyperparameter Tuning"
author: Samantha Nauman
date: "2025-04-11"
format: html
execute: 
  echo: true
---
## Data Import/Tidy/Transform
```{r}
# loading in the packages
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(ggplot2)
library(rsample)
library(visdat)
library(ggpubr)
library(patchwork)
library(skimr)
library(dials)

# loading in and merging data
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')

# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
camels <- power_full_join(camels ,by = 'gauge_id')

# clean the data
camels <- na.omit(camels)
skim(camels)
```
## Data Splitting
```{r}
# set seed to ensure random process is reproducable
set.seed(123)

# split the data
camels <- camels %>%
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test <- testing(camels_split)
```
## Feature Engineering
```{r}
# recipe to predict q_mean from training data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```
## Resampling and Model Testing
```{r}
# build resamples
camels_cv <- vfold_cv(camels_train, v = 10)

# build 3 candidate models
xgb_mod <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression") 

dt_mod <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("regression") 

rf_mod <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

# test the models
wf <- workflow_set(list(rec), list(boost = xgb_mod,
                                   dt = dt_mod,
                                   ranger = rf_mod)) %>%
  workflow_map(resamples = camels_cv,
               metrics = metric_set(mae, rsq, rmse))

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)

# model selection
```
_Model Selection Reasoning:_ Based on the models chosen and the ranked metrics from the autoplot, the random forest model seems the most fitting due to it having the lowest RMSE, indicating that the model's predictions are closest to the actual values on average. In addition, from the ranking it is ranked first for mae, rmse, and rsq out of the other metrics. 

_Selected Model Description:_ The random forest model is a "random forest" type, "regression" mode, and the "ranger" engine. I believe this model is best fit for this problem because of its simplicity combined with high predictive accuracy, making it promising for predicting logQmean. In addition, random forest models are known for their robustness against overfitting. 

## Model Tuning
```{r}
# build a model for your chosen specification
rf_tune <- rand_forest(
  mode = "regression",
  mtry = tune(),
  min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression") 

# create a workflow
rf_grid <- grid_regular(
  mtry(range = c(1,5)),
  min_n(range = c(2, 10)),
  levels = 5
)
rf_grid

wf_tune <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_tune)
wf_tune

rf_tune_results <- wf_tune %>%
  tune_grid(
    resamples = camels_cv,
    grid = rf_grid
  )
autoplot(rf_tune_results)

# define the search space/dials
dials <- extract_parameter_set_dials(wf_tune)
dials <- update(dials, mtry = mtry(range = c(1, 10)))
dials <- update(dials, min_n = min_n(range = c(2, 15)))
my_grid <- grid_space_filling(dials, size = 25)
dials

dials <- extract_parameter_set_dials(wf_tune)
dials

dials$object 

dials <- extract_parameter_set_dials(wf_tune)
dials <- finalize(dials, camels_train)
my.grid <- grid_space_filling(dials, size = 25)
my.grid

# tune the model
model_params <-  tune_grid(
    wf_tune,
    resamples = camels_cv,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)
```
_Model Tuning Description:_ From the plot of the model tuning, the first 2 rows show that as minimal node size increases, mae and rmse decreases. In the third row, as minimal node size increases, rsq also increases, indicating a positive relationship. The similarity between mae and rmse with zero outliers indicates that the random forest model was a good choice due to consistency with little to no outliers. 
```{r}
# check the skill of the tuned model
  # collect_metrics
model_params %>% collect_metrics()
```
_Collect_Metrics Description:_ From using collect_metrics to check the skill of the final tuned model, there is stable performance across the 25 tuning combinations. When gathering the average of rmse, mae, and rsq, it is very close to each individual mean, indicating that the forest performs well with the choices of the hyperparameter.  
```{r}
  # show_best
show_best(model_params, metric = "mae")
```
_Show_best Description:_ From using show_best to show the best performing model based on the mean absolute error, the first row displays that model 21 is the best, with a standard error of 0.021. In addition, model 17 displays a mtry of 49 (predictors) and a min_n of 38 (minimun node size). This means that the the random forest will randomly sample 49 different predictor variables and chooses the best split among them, and the minimum node must contain at least 38 observations before it's allowed to split. 
```{r}
  # select_best
hp_best <- select_best(model_params, metric = "mae")

  # finalize your model
final_wf <- finalize_workflow(wf_tune, hp_best)
```
## Final Model Verification
```{r}
# use last_fit to fit the finalized workflow to the original split
final_fit <- last_fit(final_wf, camels_split)

# use collect_metrics to check the performance of the final model on the TEST data
final_metrics <- collect_metrics(final_fit)
final_metrics
```
_Collect_metrics Test Interpretation:_ The results show a final rmse of 0.624 and an rsq od 0.790. Lower rmse indicated that the predicted values are close to the actual values with about a 79% of variability in the test data. This shows that the model was a strong choice by the test data metrics. Considering the rmse is lower in training data rather than the test data, the training data's predictions are closer to reality, making it slightly better. 
```{r}
# use collect_predictions to check the predictions of the final model on the test data
final_pred <- collect_predictions(final_fit) 

ggplot(final_pred, aes(x = .pred, y = logQmean)) +
  geom_point() +
  scale_color_viridis_c() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  theme_linedraw() +
  labs(title = "Predicted vs. Actual Streamflow",
       x = "Predicted Streamflow (logQmean)",
       y = "Actual Streamflow (logQmean)")
```
## Building a Map!
```{r}
# pass the final fit to the augment function to make predictions on the full, cleaned data
final_model_full <- fit(final_wf, data = camels)
predictions <- augment(final_model_full, new_data = camels)

# use mutate to calculate the residuals of the predictions (predicted - actual)^2
residuals <- predictions %>%
  mutate(residuals = (.pred - q_mean)^2)

# map predictions
pred_map <- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = .pred)) + borders("state", colour = "black", fill = NA) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c(option = "C") +
  labs(title = "Predicted Streamflow (q_mean)", color = "Predicted Value") +
  theme_minimal() +
  theme(legend.position = "bottom")

# map residuals
resid_map <- ggplot(residuals, aes(x = gauge_lon, y = gauge_lat, color = residuals)) + borders("state", colour = "black", fill = NA) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c(option = "C") +
  labs(title = "Residuals of Predictions ", color = "Residuals") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(resid_map)

# combine the two maps into one figure
combined_map <- pred_map + resid_map +
  plot_annotation(
    title = "Predictions Across CONUS",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5)
    )
  )
print(combined_map)
```
