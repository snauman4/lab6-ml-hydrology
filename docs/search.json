[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggplot2)\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#data-splitting",
    "href": "lab6.html#data-splitting",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n# set a new seed for reproducible\nset.seed(123456) # new sequence\n\n# create initial 75% training and 25% testing split and extract the sets\ncamels_strata &lt;- initial_split(camels, prop = .75)\n\ntrain_camels &lt;- training(camels_strata)\ntest_camels &lt;- testing(camels_strata)\n\n# build a 10-fold CV dataset\ncamels_folds &lt;-\n  vfold_cv(train_camels, v = 10)"
  },
  {
    "objectID": "lab6.html#recipe",
    "href": "lab6.html#recipe",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Recipe",
    "text": "Recipe\n\n# define a formula you want to use\nformula &lt;- logQmean ~ p_mean + aridity + high_prec_dur\n\n# build a recipe\ntrain_camels &lt;- na.omit(train_camels)\nrec_camels &lt;-  recipe(logQmean ~ p_mean + aridity + high_prec_dur, data = train_camels) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;%\n  step_naomit(all_predictors(), all_outcomes()) %&gt;%\n  step_zv(all_predictors()) \n\n# prep the data\nbaked_camels &lt;- prep(rec_camels, train_camels) %&gt;%\n  bake(new_data = NULL)\n\n# check the recipe (should be zero)\nsum(is.na(baked_camels))\n\n[1] 0\n\nsum(is.infinite(as.matrix(baked_camels))) \n\n[1] 0\n\n\n\nThe formula was chose was based on the inclusion of the predictor variables that I believe influence mean daily discharge: p_mean, aridity, and logQmean. Precipitation adds water to the system, while aridity indicates how dry an area is (drier areas usually have lower logQmean). I also expect that more frequent heavy rain events (high_prec_dur) will lead to higher average discharge."
  },
  {
    "objectID": "lab6.html#define-3-models",
    "href": "lab6.html#define-3-models",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Define 3 Models",
    "text": "Define 3 Models\n\n# define a random forest model\nq4_rf_mod &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# 2 other models of choice\nq4_xgb_mod &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\") \n\nq4_lm_mod &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "lab6.html#workflow-set",
    "href": "lab6.html#workflow-set",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Workflow Set",
    "text": "Workflow Set\n\n# create workflow objects, add the recipe, add the models\nq4_rf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_camels) %&gt;%\n  add_model(q4_rf_mod)\n\nq4_xgb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_camels) %&gt;%\n  add_model(q4_xgb_mod)\n\nq4_lm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_camels) %&gt;%\n  add_model(q4_lm_mod) \n\n# fit the model to the resamples\nrf_results &lt;- fit_resamples(q4_rf_wf, resamples = camels_folds)\nxgb_results &lt;- fit_resamples(q4_xgb_wf, resamples = camels_folds) \nlm_results &lt;- fit_resamples(q4_lm_wf, resamples = camels_folds)"
  },
  {
    "objectID": "lab6.html#evaluation",
    "href": "lab6.html#evaluation",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Evaluation",
    "text": "Evaluation\n\n# use autoplot and rank_results to compare the models\nq4_wf &lt;- workflow_set(list(rec_camels), list(q4_rf_mod, q4_xgb_mod, q4_lm_mod)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(q4_wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     1\n2 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0249    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.769  0.0261    10 recipe       rand…     2\n\n\n\nOut of the random forest, linear, and xgboost model, I think that the random forest model is the strongest performer because it shows the lowest RMSE and the highest RSQ compared to the others, capturing the relationship between predictors and response very well. These metrics indicate that the random forest’s predictions are close to the actual values and explain a large variance in the data. This means it consistently makes accurate predictions and captures the underlying patterns in the dataset better than the other models."
  },
  {
    "objectID": "lab6.html#extract-and-evaluate",
    "href": "lab6.html#extract-and-evaluate",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Extract and Evaluate",
    "text": "Extract and Evaluate\n\n# build a workflow with favorite model, recipe, and training data\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(rec_camels) %&gt;%\n  add_model(q4_rf_mod) %&gt;%\n  fit(data = train_camels) # use fit to fit all training data to the model\n\n# use augment to make preditions on the test data\nfinal_wf_data &lt;- augment(final_wf, new_data = test_camels)\n\n#create a plot of the observed vs predicted values\nggplot(final_wf_data, aes(x = .pred, y = logQmean, colour = logQmean)) +\n  scale_color_gradient2(low = \"blue3\", mid = \"yellow\", high = \"chartreuse\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  labs(title = \"Random Forest Model: Observed vs. Predicted\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow?\")\n\n\n\n\n\n\n\n\nThese results suggest that the random forest model performs well on predicting logQmean based on the predictors chosen. Most of the points are along the 1:1 line, indicating that the model’s predicted values match the observed values closely, accurately capturing the relationship between the predictors and logQmean effectively."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "",
    "text": "# loading in the packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggplot2)\nlibrary(rsample)\nlibrary(visdat)\nlibrary(ggpubr)\nlibrary(patchwork)\nlibrary(skimr)\nlibrary(dials)\n\n# loading in and merging data\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n# clean the data\ncamels &lt;- na.omit(camels)\nskim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n507\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1\n8\n8\n0\n507\n0\n\n\nhigh_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n0\n1\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1\n3.15\n1.49\n0.64\n2.20\n3.05\n3.70\n8.94\n▅▇▂▁▁\n\n\npet_mean\n0\n1\n2.80\n0.56\n1.94\n2.37\n2.70\n3.16\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1\n-0.06\n0.56\n-1.44\n-0.41\n0.08\n0.27\n0.92\n▂▃▃▇▃\n\n\nfrac_snow\n0\n1\n0.19\n0.21\n0.00\n0.04\n0.11\n0.26\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1\n1.13\n0.67\n0.22\n0.73\n0.89\n1.45\n5.21\n▇▃▁▁▁\n\n\nhigh_prec_freq\n0\n1\n21.00\n4.80\n7.90\n18.45\n22.20\n24.55\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1\n1.37\n0.19\n1.10\n1.22\n1.32\n1.47\n2.07\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1\n256.92\n36.76\n169.90\n233.57\n259.15\n286.75\n348.70\n▂▅▇▆▁\n\n\nlow_prec_dur\n0\n1\n6.29\n3.46\n2.79\n4.42\n5.14\n7.21\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1\n0.73\n0.19\n0.30\n0.58\n0.76\n0.90\n1.00\n▂▅▅▆▇\n\n\nglim_2nd_class_frac\n0\n1\n0.19\n0.13\n0.00\n0.08\n0.19\n0.30\n0.49\n▇▆▆▅▂\n\n\ncarbonate_rocks_frac\n0\n1\n0.13\n0.26\n0.00\n0.00\n0.00\n0.09\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n0\n1\n0.12\n0.06\n0.01\n0.07\n0.12\n0.17\n0.28\n▆▆▇▆▁\n\n\ngeol_permeability\n0\n1\n-13.86\n1.13\n-16.50\n-14.66\n-13.90\n-13.03\n-10.97\n▂▃▇▅▁\n\n\nsoil_depth_pelletier\n0\n1\n9.45\n15.25\n0.27\n1.00\n1.20\n6.99\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1\n1.28\n0.27\n0.40\n1.08\n1.42\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.59\n▁▇▂▁▁\n\n\nsoil_conductivity\n0\n1\n1.67\n1.39\n0.45\n0.89\n1.34\n1.89\n10.91\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1\n0.52\n0.15\n0.09\n0.41\n0.53\n0.64\n0.88\n▁▃▆▇▁\n\n\nsand_frac\n0\n1\n35.69\n15.47\n8.18\n24.62\n34.60\n43.77\n91.16\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1\n33.30\n12.82\n4.13\n23.53\n33.65\n42.76\n67.77\n▂▇▇▅▁\n\n\nclay_frac\n0\n1\n20.35\n9.81\n2.08\n13.82\n18.88\n26.58\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.71\n▇▁▁▁▁\n\n\norganic_frac\n0\n1\n0.45\n2.97\n0.00\n0.00\n0.00\n0.00\n39.37\n▇▁▁▁▁\n\n\nother_frac\n0\n1\n10.88\n16.94\n0.00\n0.00\n2.25\n15.48\n89.87\n▇▂▁▁▁\n\n\ngauge_lat\n0\n1\n39.55\n5.21\n27.05\n36.23\n39.35\n43.96\n48.66\n▂▃▇▅▆\n\n\ngauge_lon\n0\n1\n-98.08\n16.26\n-124.39\n-112.03\n-97.04\n-83.40\n-67.94\n▇▆▇▇▅\n\n\nelev_mean\n0\n1\n842.40\n824.66\n21.75\n278.50\n492.06\n1055.30\n3457.46\n▇▂▁▁▁\n\n\nslope_mean\n0\n1\n50.12\n48.29\n0.82\n7.97\n36.17\n77.62\n255.69\n▇▃▂▁▁\n\n\narea_gages2\n0\n1\n854.89\n1829.26\n4.03\n151.12\n383.82\n855.14\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1\n870.19\n1837.52\n4.10\n164.23\n397.25\n861.21\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1\n0.61\n0.38\n0.00\n0.20\n0.75\n0.97\n1.00\n▅▁▂▂▇\n\n\nlai_max\n0\n1\n3.00\n1.52\n0.37\n1.63\n2.75\n4.60\n5.58\n▅▇▃▅▇\n\n\nlai_diff\n0\n1\n2.27\n1.32\n0.15\n1.07\n2.06\n3.49\n4.82\n▇▇▆▃▆\n\n\ngvf_max\n0\n1\n0.70\n0.17\n0.18\n0.58\n0.75\n0.86\n0.92\n▁▂▃▃▇\n\n\ngvf_diff\n0\n1\n0.31\n0.15\n0.03\n0.18\n0.29\n0.45\n0.65\n▅▇▅▇▂\n\n\ndom_land_cover_frac\n0\n1\n0.80\n0.19\n0.31\n0.64\n0.84\n0.99\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n0\n1\n0.18\n0.03\n0.12\n0.16\n0.18\n0.19\n0.25\n▃▅▇▂▂\n\n\nroot_depth_99\n0\n1\n1.81\n0.30\n1.50\n1.51\n1.79\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n0\n1\n1.46\n1.61\n0.00\n0.49\n1.00\n1.72\n9.50\n▇▁▁▁▁\n\n\nrunoff_ratio\n0\n1\n0.38\n0.24\n0.00\n0.22\n0.34\n0.51\n1.36\n▇▇▃▁▁\n\n\nslope_fdc\n0\n1\n1.19\n0.53\n0.00\n0.81\n1.24\n1.56\n2.50\n▃▆▇▆▁\n\n\nbaseflow_index\n0\n1\n0.49\n0.17\n0.01\n0.39\n0.51\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n0\n1\n1.86\n0.80\n-0.64\n1.33\n1.69\n2.25\n6.24\n▁▇▃▁▁\n\n\nq5\n0\n1\n0.17\n0.25\n0.00\n0.01\n0.08\n0.22\n1.77\n▇▁▁▁▁\n\n\nq95\n0\n1\n4.98\n5.23\n0.00\n1.67\n3.46\n6.29\n31.23\n▇▂▁▁▁\n\n\nhigh_q_freq\n0\n1\n27.15\n30.51\n0.00\n6.92\n16.15\n38.60\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n0\n1\n7.62\n10.98\n0.00\n1.87\n3.24\n8.82\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n0\n1\n111.13\n86.38\n0.00\n35.35\n101.35\n173.10\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n0\n1\n23.38\n23.12\n0.00\n10.31\n16.92\n28.20\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n0\n1\n0.04\n0.13\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n0\n1\n185.49\n34.66\n112.25\n162.07\n177.80\n212.10\n287.75\n▂▇▅▃▁"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-importtidytransform",
    "href": "hyperparameter-tuning.html#data-importtidytransform",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "",
    "text": "# loading in the packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggplot2)\nlibrary(rsample)\nlibrary(visdat)\nlibrary(ggpubr)\nlibrary(patchwork)\nlibrary(skimr)\nlibrary(dials)\n\n# loading in and merging data\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n# clean the data\ncamels &lt;- na.omit(camels)\nskim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n507\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1\n8\n8\n0\n507\n0\n\n\nhigh_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n0\n1\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1\n3.15\n1.49\n0.64\n2.20\n3.05\n3.70\n8.94\n▅▇▂▁▁\n\n\npet_mean\n0\n1\n2.80\n0.56\n1.94\n2.37\n2.70\n3.16\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1\n-0.06\n0.56\n-1.44\n-0.41\n0.08\n0.27\n0.92\n▂▃▃▇▃\n\n\nfrac_snow\n0\n1\n0.19\n0.21\n0.00\n0.04\n0.11\n0.26\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1\n1.13\n0.67\n0.22\n0.73\n0.89\n1.45\n5.21\n▇▃▁▁▁\n\n\nhigh_prec_freq\n0\n1\n21.00\n4.80\n7.90\n18.45\n22.20\n24.55\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1\n1.37\n0.19\n1.10\n1.22\n1.32\n1.47\n2.07\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1\n256.92\n36.76\n169.90\n233.57\n259.15\n286.75\n348.70\n▂▅▇▆▁\n\n\nlow_prec_dur\n0\n1\n6.29\n3.46\n2.79\n4.42\n5.14\n7.21\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1\n0.73\n0.19\n0.30\n0.58\n0.76\n0.90\n1.00\n▂▅▅▆▇\n\n\nglim_2nd_class_frac\n0\n1\n0.19\n0.13\n0.00\n0.08\n0.19\n0.30\n0.49\n▇▆▆▅▂\n\n\ncarbonate_rocks_frac\n0\n1\n0.13\n0.26\n0.00\n0.00\n0.00\n0.09\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n0\n1\n0.12\n0.06\n0.01\n0.07\n0.12\n0.17\n0.28\n▆▆▇▆▁\n\n\ngeol_permeability\n0\n1\n-13.86\n1.13\n-16.50\n-14.66\n-13.90\n-13.03\n-10.97\n▂▃▇▅▁\n\n\nsoil_depth_pelletier\n0\n1\n9.45\n15.25\n0.27\n1.00\n1.20\n6.99\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1\n1.28\n0.27\n0.40\n1.08\n1.42\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.59\n▁▇▂▁▁\n\n\nsoil_conductivity\n0\n1\n1.67\n1.39\n0.45\n0.89\n1.34\n1.89\n10.91\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1\n0.52\n0.15\n0.09\n0.41\n0.53\n0.64\n0.88\n▁▃▆▇▁\n\n\nsand_frac\n0\n1\n35.69\n15.47\n8.18\n24.62\n34.60\n43.77\n91.16\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1\n33.30\n12.82\n4.13\n23.53\n33.65\n42.76\n67.77\n▂▇▇▅▁\n\n\nclay_frac\n0\n1\n20.35\n9.81\n2.08\n13.82\n18.88\n26.58\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.71\n▇▁▁▁▁\n\n\norganic_frac\n0\n1\n0.45\n2.97\n0.00\n0.00\n0.00\n0.00\n39.37\n▇▁▁▁▁\n\n\nother_frac\n0\n1\n10.88\n16.94\n0.00\n0.00\n2.25\n15.48\n89.87\n▇▂▁▁▁\n\n\ngauge_lat\n0\n1\n39.55\n5.21\n27.05\n36.23\n39.35\n43.96\n48.66\n▂▃▇▅▆\n\n\ngauge_lon\n0\n1\n-98.08\n16.26\n-124.39\n-112.03\n-97.04\n-83.40\n-67.94\n▇▆▇▇▅\n\n\nelev_mean\n0\n1\n842.40\n824.66\n21.75\n278.50\n492.06\n1055.30\n3457.46\n▇▂▁▁▁\n\n\nslope_mean\n0\n1\n50.12\n48.29\n0.82\n7.97\n36.17\n77.62\n255.69\n▇▃▂▁▁\n\n\narea_gages2\n0\n1\n854.89\n1829.26\n4.03\n151.12\n383.82\n855.14\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1\n870.19\n1837.52\n4.10\n164.23\n397.25\n861.21\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1\n0.61\n0.38\n0.00\n0.20\n0.75\n0.97\n1.00\n▅▁▂▂▇\n\n\nlai_max\n0\n1\n3.00\n1.52\n0.37\n1.63\n2.75\n4.60\n5.58\n▅▇▃▅▇\n\n\nlai_diff\n0\n1\n2.27\n1.32\n0.15\n1.07\n2.06\n3.49\n4.82\n▇▇▆▃▆\n\n\ngvf_max\n0\n1\n0.70\n0.17\n0.18\n0.58\n0.75\n0.86\n0.92\n▁▂▃▃▇\n\n\ngvf_diff\n0\n1\n0.31\n0.15\n0.03\n0.18\n0.29\n0.45\n0.65\n▅▇▅▇▂\n\n\ndom_land_cover_frac\n0\n1\n0.80\n0.19\n0.31\n0.64\n0.84\n0.99\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n0\n1\n0.18\n0.03\n0.12\n0.16\n0.18\n0.19\n0.25\n▃▅▇▂▂\n\n\nroot_depth_99\n0\n1\n1.81\n0.30\n1.50\n1.51\n1.79\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n0\n1\n1.46\n1.61\n0.00\n0.49\n1.00\n1.72\n9.50\n▇▁▁▁▁\n\n\nrunoff_ratio\n0\n1\n0.38\n0.24\n0.00\n0.22\n0.34\n0.51\n1.36\n▇▇▃▁▁\n\n\nslope_fdc\n0\n1\n1.19\n0.53\n0.00\n0.81\n1.24\n1.56\n2.50\n▃▆▇▆▁\n\n\nbaseflow_index\n0\n1\n0.49\n0.17\n0.01\n0.39\n0.51\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n0\n1\n1.86\n0.80\n-0.64\n1.33\n1.69\n2.25\n6.24\n▁▇▃▁▁\n\n\nq5\n0\n1\n0.17\n0.25\n0.00\n0.01\n0.08\n0.22\n1.77\n▇▁▁▁▁\n\n\nq95\n0\n1\n4.98\n5.23\n0.00\n1.67\n3.46\n6.29\n31.23\n▇▂▁▁▁\n\n\nhigh_q_freq\n0\n1\n27.15\n30.51\n0.00\n6.92\n16.15\n38.60\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n0\n1\n7.62\n10.98\n0.00\n1.87\n3.24\n8.82\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n0\n1\n111.13\n86.38\n0.00\n35.35\n101.35\n173.10\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n0\n1\n23.38\n23.12\n0.00\n10.31\n16.92\n28.20\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n0\n1\n0.04\n0.13\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n0\n1\n185.49\n34.66\n112.25\n162.07\n177.80\n212.10\n287.75\n▂▇▅▃▁"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-splitting",
    "href": "hyperparameter-tuning.html#data-splitting",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n# set seed to ensure random process is reproducable\nset.seed(123)\n\n# split the data\ncamels &lt;- camels %&gt;%\n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test &lt;- testing(camels_split)"
  },
  {
    "objectID": "hyperparameter-tuning.html#feature-engineering",
    "href": "hyperparameter-tuning.html#feature-engineering",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# recipe to predict q_mean from training data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())"
  },
  {
    "objectID": "hyperparameter-tuning.html#resampling-and-model-testing",
    "href": "hyperparameter-tuning.html#resampling-and-model-testing",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "Resampling and Model Testing",
    "text": "Resampling and Model Testing\n\n# build resamples\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# build 3 candidate models\nxgb_mod &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\") \n\ndt_mod &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\") \n\nrf_mod &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# test the models\nwf &lt;- workflow_set(list(rec), list(boost = xgb_mod,\n                                   dt = dt_mod,\n                                   ranger = rf_mod)) %&gt;%\n  workflow_map(resamples = camels_cv,\n               metrics = metric_set(mae, rsq, rmse))\n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 9 × 9\n  wflow_id      .config     .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_ranger Preprocess… mae     0.396  0.0196    10 recipe       rand…     1\n2 recipe_ranger Preprocess… rmse    0.594  0.0329    10 recipe       rand…     1\n3 recipe_ranger Preprocess… rsq     0.762  0.0289    10 recipe       rand…     1\n4 recipe_dt     Preprocess… mae     0.438  0.0213    10 recipe       deci…     2\n5 recipe_dt     Preprocess… rmse    0.609  0.0282    10 recipe       deci…     2\n6 recipe_dt     Preprocess… rsq     0.747  0.0270    10 recipe       deci…     2\n7 recipe_boost  Preprocess… mae     0.420  0.0152    10 recipe       boos…     3\n8 recipe_boost  Preprocess… rmse    0.645  0.0238    10 recipe       boos…     3\n9 recipe_boost  Preprocess… rsq     0.728  0.0203    10 recipe       boos…     3\n\n# model selection\n\nModel Selection Reasoning: Based on the models chosen and the ranked metrics from the autoplot, the random forest model seems the most fitting due to it having the lowest RMSE, indicating that the model’s predictions are closest to the actual values on average. In addition, from the ranking it is ranked first for mae, rmse, and rsq out of the other metrics.\nSelected Model Description: The random forest model is a “random forest” type, “regression” mode, and the “ranger” engine. I believe this model is best fit for this problem because of its simplicity combined with high predictive accuracy, making it promising for predicting logQmean. In addition, random forest models are known for their robustness against overfitting."
  },
  {
    "objectID": "hyperparameter-tuning.html#model-tuning",
    "href": "hyperparameter-tuning.html#model-tuning",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "Model Tuning",
    "text": "Model Tuning\n\n# build a model for your chosen specification\nrf_tune &lt;- rand_forest(\n  mode = \"regression\",\n  mtry = tune(),\n  min_n = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\") \n\n# create a workflow\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1,5)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\nrf_grid\n\n# A tibble: 25 × 2\n    mtry min_n\n   &lt;int&gt; &lt;int&gt;\n 1     1     2\n 2     2     2\n 3     3     2\n 4     4     2\n 5     5     2\n 6     1     4\n 7     2     4\n 8     3     4\n 9     4     4\n10     5     4\n# ℹ 15 more rows\n\nwf_tune &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_tune)\nwf_tune\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_log()\n• step_interact()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: ranger \n\nrf_tune_results &lt;- wf_tune %&gt;%\n  tune_grid(\n    resamples = camels_cv,\n    grid = rf_grid\n  )\n\n→ A | warning: 4 columns were requested but there were 3 predictors in the data. 3 will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: 5 columns were requested but there were 3 predictors in the data. 3 will be used.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x5   B: x4\nThere were issues with some computations   A: x10   B: x9\nThere were issues with some computations   A: x15   B: x14\nThere were issues with some computations   A: x20   B: x19\nThere were issues with some computations   A: x26   B: x25\nThere were issues with some computations   A: x31   B: x30\nThere were issues with some computations   A: x36   B: x35\nThere were issues with some computations   A: x41   B: x40\nThere were issues with some computations   A: x46   B: x46\nThere were issues with some computations   A: x50   B: x50\n\nautoplot(rf_tune_results)\n\n\n\n\n\n\n\n# define the search space/dials\ndials &lt;- extract_parameter_set_dials(wf_tune)\ndials &lt;- update(dials, mtry = mtry(range = c(1, 10)))\ndials &lt;- update(dials, min_n = min_n(range = c(2, 15)))\nmy_grid &lt;- grid_space_filling(dials, size = 25)\ndials\n\nCollection of 2 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[+]\n      min_n min_n nparam[+]\n\ndials &lt;- extract_parameter_set_dials(wf_tune)\ndials\n\nCollection of 2 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\ndials$object \n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [1, ?]\n\n[[2]]\nMinimal Node Size (quantitative)\nRange: [2, 40]\n\ndials &lt;- extract_parameter_set_dials(wf_tune)\ndials &lt;- finalize(dials, camels_train)\nmy.grid &lt;- grid_space_filling(dials, size = 25)\nmy.grid\n\n# A tibble: 25 × 2\n    mtry min_n\n   &lt;int&gt; &lt;int&gt;\n 1     1    24\n 2     3    13\n 3     5    33\n 4     8     5\n 5    10    19\n 6    13    27\n 7    15    36\n 8    17    11\n 9    20     3\n10    22    21\n# ℹ 15 more rows\n\n# tune the model\nmodel_params &lt;-  tune_grid(\n    wf_tune,\n    resamples = camels_cv,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\n\n→ A | warning: 5 columns were requested but there were 3 predictors in the data. 3 will be used.\n→ B | warning: 8 columns were requested but there were 3 predictors in the data. 3 will be used.\n→ C | warning: 10 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ D | warning: 13 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ E | warning: 15 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ F | warning: 17 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ G | warning: 20 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ H | warning: 22 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ I | warning: 25 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ J | warning: 27 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ K | warning: 30 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ L | warning: 32 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ M | warning: 34 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ N | warning: 37 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ O | warning: 39 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ P | warning: 42 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ Q | warning: 44 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ R | warning: 46 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ S | warning: 49 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ T | warning: 51 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ U | warning: 54 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ V | warning: 56 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ W | warning: 59 columns were requested but there were 3 predictors in the data. 3 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\nThere were issues with some computations   A: x2   B: x2   C: x2   D: x2   E: x…\nThere were issues with some computations   A: x3   B: x3   C: x3   D: x3   E: x…\nThere were issues with some computations   A: x4   B: x4   C: x4   D: x4   E: x…\nThere were issues with some computations   A: x6   B: x5   C: x5   D: x5   E: x…\nThere were issues with some computations   A: x7   B: x7   C: x7   D: x7   E: x…\nThere were issues with some computations   A: x8   B: x8   C: x8   D: x8   E: x…\nThere were issues with some computations   A: x9   B: x9   C: x9   D: x9   E: x…\nThere were issues with some computations   A: x10   B: x10   C: x10   D: x10   …\nThere were issues with some computations   A: x10   B: x10   C: x10   D: x10   …\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\nModel Tuning Description: From the plot of the model tuning, the first 2 rows show that as minimal node size increases, mae and rmse decreases. In the third row, as minimal node size increases, rsq also increases, indicating a positive relationship. The similarity between mae and rmse with zero outliers indicates that the random forest model was a good choice due to consistency with little to no outliers.\n\n# check the skill of the tuned model\n  # collect_metrics\nmodel_params %&gt;% collect_metrics()\n\n# A tibble: 75 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1    24 mae     standard   0.381    10  0.0177 Preprocessor1_Model01\n 2     1    24 rmse    standard   0.569    10  0.0292 Preprocessor1_Model01\n 3     1    24 rsq     standard   0.780    10  0.0250 Preprocessor1_Model01\n 4     3    13 mae     standard   0.390    10  0.0195 Preprocessor1_Model02\n 5     3    13 rmse    standard   0.589    10  0.0325 Preprocessor1_Model02\n 6     3    13 rsq     standard   0.763    10  0.0287 Preprocessor1_Model02\n 7     5    33 mae     standard   0.381    10  0.0177 Preprocessor1_Model03\n 8     5    33 rmse    standard   0.569    10  0.0288 Preprocessor1_Model03\n 9     5    33 rsq     standard   0.777    10  0.0264 Preprocessor1_Model03\n10     8     5 mae     standard   0.400    10  0.0202 Preprocessor1_Model04\n# ℹ 65 more rows\n\n\nCollect_Metrics Description: From using collect_metrics to check the skill of the final tuned model, there is stable performance across the 25 tuning combinations. When gathering the average of rmse, mae, and rsq, it is very close to each individual mean, indicating that the forest performs well with the choices of the hyperparameter.\n\n  # show_best\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    25    30 mae     standard   0.380    10  0.0184 Preprocessor1_Model11\n2    49    38 mae     standard   0.381    10  0.0175 Preprocessor1_Model21\n3    13    27 mae     standard   0.381    10  0.0184 Preprocessor1_Model06\n4    59    32 mae     standard   0.381    10  0.0176 Preprocessor1_Model25\n5     1    24 mae     standard   0.381    10  0.0177 Preprocessor1_Model01\n\n\nShow_best Description: From using show_best to show the best performing model based on the mean absolute error, the first row displays that model 21 is the best, with a standard error of 0.021. In addition, model 17 displays a mtry of 49 (predictors) and a min_n of 38 (minimun node size). This means that the the random forest will randomly sample 49 different predictor variables and chooses the best split among them, and the minimum node must contain at least 38 observations before it’s allowed to split.\n\n  # select_best\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n  # finalize your model\nfinal_wf &lt;- finalize_workflow(wf_tune, hp_best)"
  },
  {
    "objectID": "hyperparameter-tuning.html#final-model-verification",
    "href": "hyperparameter-tuning.html#final-model-verification",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "Final Model Verification",
    "text": "Final Model Verification\n\n# use last_fit to fit the finalized workflow to the original split\nfinal_fit &lt;- last_fit(final_wf, camels_split)\n\n→ A | warning: 25 columns were requested but there were 3 predictors in the data. 3 will be used.\n\n# use collect_metrics to check the performance of the final model on the TEST data\nfinal_metrics &lt;- collect_metrics(final_fit)\nfinal_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.616 Preprocessor1_Model1\n2 rsq     standard       0.796 Preprocessor1_Model1\n\n\nCollect_metrics Test Interpretation: The results show a final rmse of 0.624 and an rsq od 0.790. Lower rmse indicated that the predicted values are close to the actual values with about a 79% of variability in the test data. This shows that the model was a strong choice by the test data metrics. Considering the rmse is lower in training data rather than the test data, the training data’s predictions are closer to reality, making it slightly better.\n\n# use collect_predictions to check the predictions of the final model on the test data\nfinal_pred &lt;- collect_predictions(final_fit) \n\nggplot(final_pred, aes(x = .pred, y = logQmean)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n  theme_linedraw() +\n  labs(title = \"Predicted vs. Actual Streamflow\",\n       x = \"Predicted Streamflow (logQmean)\",\n       y = \"Actual Streamflow (logQmean)\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "hyperparameter-tuning.html#building-a-map",
    "href": "hyperparameter-tuning.html#building-a-map",
    "title": "Lab 8: Machine Learning in Hydrology",
    "section": "Building a Map!",
    "text": "Building a Map!\n\n# pass the final fit to the augment function to make predictions on the full, cleaned data\nfinal_model_full &lt;- fit(final_wf, data = camels)\n\nWarning: 25 columns were requested but there were 3 predictors in the data. 3\nwill be used.\n\npredictions &lt;- augment(final_model_full, new_data = camels)\n\n# use mutate to calculate the residuals of the predictions (predicted - actual)^2\nresiduals &lt;- predictions %&gt;%\n  mutate(residuals = (.pred - q_mean)^2)\n\n# map predictions\npred_map &lt;- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = .pred)) + borders(\"state\", colour = \"black\", fill = NA) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Predicted Streamflow (q_mean)\", color = \"Predicted Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n# map residuals\nresid_map &lt;- ggplot(residuals, aes(x = gauge_lon, y = gauge_lat, color = residuals)) + borders(\"state\", colour = \"black\", fill = NA) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Residuals of Predictions \", color = \"Residuals\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(resid_map)\n\n\n\n\n\n\n\n# combine the two maps into one figure\ncombined_map &lt;- pred_map + resid_map +\n  plot_annotation(\n    title = \"Predictions Across CONUS\",\n    theme = theme(\n      plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5)\n    )\n  )\nprint(combined_map)"
  }
]